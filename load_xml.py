"""
================================================================================
Congressional Financial Disclosure Analysis Tool
================================================================================

LEGAL DISCLAIMER - PLEASE READ CAREFULLY:

THIS SOFTWARE IS PROVIDED FOR INFORMATIONAL AND EDUCATIONAL PURPOSES ONLY.

THIS TOOL IS NOT INVESTMENT ADVICE, FINANCIAL ADVICE, LEGAL ADVICE, OR ANY
FORM OF PROFESSIONAL ADVICE. The information generated by this software should
NOT be used as the sole basis for any investment, financial, or legal decisions.

The developers, contributors, and distributors of this software:
- Make NO WARRANTIES, express or implied, about the accuracy, completeness,
  or reliability of any analysis, data, or information generated
- Accept NO LIABILITY for any losses, damages, or consequences arising from
  the use of this software or reliance on its output
- DO NOT guarantee the correctness of data extraction, pattern identification,
  or any analysis performed by AI models
- Are NOT responsible for any investment decisions made based on information
  processed by this tool

Financial disclosure data is publicly available but may contain errors,
omissions, or require legal interpretation. Users must:
- Verify all information independently
- Consult qualified financial, legal, and tax professionals before making
  any investment decisions
- Understand that past trading activity does not predict future performance
- Recognize that AI-generated analysis may contain errors or misinterpretations

By using this software, you acknowledge and agree to these terms and accept
full responsibility for your own investment decisions.

================================================================================

DESCRIPTION:

This tool retrieves and processes publicly available Congressional financial
disclosure documents. It downloads the official XML index, builds the PDF
document URLs, converts the PDFs to images, and submits those images to a
vision/OCR analysis service to extract structured information (for example,
company names, ticker symbols, and high-level transaction details). The script
can batch multiple filings and produce a consolidated summary.

KEY CAPABILITIES:
- Automated retrieval of financial disclosure XML data and PDF documents
- Batch processing of multiple filings simultaneously for efficiency
- Vision/OCR-based content extraction from PDF documents
- Extraction of company names, ticker symbols, and transaction details
- Consolidated analysis across multiple disclosure documents
- Robust handling of differing document layouts

TECHNICAL FEATURES:
- Multi-threaded PDF fetching for parallel processing
- Automatic XML data refresh when stale (>2 days old)
- Base64 image encoding for API transmission
- Randomized user-agent rotation for robust HTTP requests
- Comprehensive error handling and logging
- Test mode for validation without API consumption

USAGE:
    python3 load_xml.py              # Full analysis (requires OPENAI_API_KEY)
    python3 load_xml.py --test       # Test mode (no API key needed)

REQUIREMENTS:
    - OPENAI_API_KEY environment variable (for full analysis mode)
    - pdf2image package (pip install pdf2image)
    - colorlog package (pip install colorlog)
    - fake-useragent package (pip install fake-useragent)
    - pandas, requests, tabulate packages
    - 2025FD.xml file (auto-downloaded if missing or outdated)

DATA SOURCE:
    House of Representatives Financial Disclosures
    https://disclosures-clerk.house.gov/

Version: 1.0.0
License: MIT
"""

import base64
import concurrent.futures
import io
import logging
import os
import sys
import tempfile
import time
import zipfile
from pathlib import Path
from typing import List, Optional, Tuple

import colorlog
import pandas as pd
import requests
from fake_useragent import UserAgent
from tabulate import tabulate

try:
    from pdf2image import convert_from_bytes

    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Set up colored console logging for improved readability
# Levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
logger = logging.getLogger(__name__)
handler = colorlog.StreamHandler()
handler.setFormatter(
    colorlog.ColoredFormatter("%(log_color)s%(levelname)s: %(message)s")
)
logger.addHandler(handler)
logger.setLevel(logging.INFO)


# =============================================================================
# CONFIGURATION CONSTANTS
# =============================================================================
# File and URL Configuration
DEFAULT_XML_FILE = "2025FD.xml"  # Local XML metadata file
DEFAULT_BASE_URL = "https://disclosures-clerk.house.gov/public_disc/financial-pdfs/"
ZIP_URL = "https://disclosures-clerk.house.gov/public_disc/financial-pdfs/2025FD.zip"

# Processing Configuration
DEFAULT_DAYS = 30  # Time window for filtering recent filings (days)
DEFAULT_TOP_N = None  # Number of results to display (None = show all)
MAX_XML_AGE_DAYS = 2  # Maximum XML file age before auto-refresh (days)

# Format Configuration
DATE_FORMAT = "%m/%d/%Y"  # Date format used in XML data
OPENAI_MODEL = "gpt-4.1-mini"  # OpenAI vision-capable model for analysis


def download_and_extract_xml() -> None:
    """
    Download and extract XML metadata file from official House source.

    Implements intelligent caching: only downloads fresh data when local XML file
    is missing or exceeds MAX_XML_AGE_DAYS age threshold. This minimizes network
    requests while ensuring data freshness.

    Process:
        1. Check local XML file existence and modification time
        2. If fresh enough, skip download and use cached version
        3. If stale/missing, download ZIP archive from official source
        4. Extract XML file to current working directory
        5. Clean up temporary files

    Raises:
        requests.exceptions.RequestException: If download fails
        zipfile.BadZipFile: If ZIP file is corrupted
        ValueError: If no XML file found in archive

    Note:
        Falls back to existing XML file if download fails (with warning)
    """
    xml_path = Path(DEFAULT_XML_FILE)

    # Check if XML file exists and validate its freshness
    if xml_path.exists():
        # Calculate file age in days from modification timestamp
        file_age_days = (time.time() - xml_path.stat().st_mtime) / (24 * 3600)
        if file_age_days < MAX_XML_AGE_DAYS:
            logger.info(f"Using cached XML file ({file_age_days:.1f} days old)")
            return
        logger.info(
            f"XML file stale ({file_age_days:.1f} days old), downloading fresh data..."
        )
    else:
        logger.info("XML file not found locally, initiating download...")

    try:
        # Download ZIP archive from official House disclosure server
        logger.info(f"Downloading ZIP archive from: {ZIP_URL}")
        ua = UserAgent()
        headers = {"User-Agent": ua.random}  # Rotate user agent to avoid blocking
        response = requests.get(ZIP_URL, headers=headers, timeout=60)
        response.raise_for_status()  # Raise exception for HTTP errors

        # Write ZIP content to temporary file for extraction
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as temp_zip:
            temp_zip.write(response.content)
            temp_zip_path = temp_zip.name

        # Extract XML metadata file from ZIP archive
        logger.info("Extracting XML metadata file from archive...")
        with zipfile.ZipFile(temp_zip_path, "r") as zip_ref:
            # Locate XML file within archive (should be exactly one)
            xml_files = [f for f in zip_ref.namelist() if f.endswith(".xml")]
            if not xml_files:
                raise ValueError("No XML file found in ZIP archive")

            # Extract first (and typically only) XML file to current directory
            xml_filename = xml_files[0]
            with (
                zip_ref.open(xml_filename) as source,
                open(DEFAULT_XML_FILE, "wb") as target,
            ):
                target.write(source.read())

        # Clean up temporary ZIP file to free disk space
        os.unlink(temp_zip_path)

        logger.info(f"Successfully extracted and saved: {DEFAULT_XML_FILE}")

    except requests.exceptions.RequestException as e:
        logger.error(f"Network error during XML download: {e}")
        if xml_path.exists():
            logger.warning("Falling back to existing (possibly stale) XML file")
        else:
            raise RuntimeError(
                "Cannot proceed: No XML data available and download failed"
            ) from e
    except (zipfile.BadZipFile, ValueError) as e:
        logger.error(f"Error processing ZIP archive: {e}")
        if xml_path.exists():
            logger.warning("Falling back to existing (possibly stale) XML file")
        else:
            raise RuntimeError(
                "Cannot proceed: No XML data available and extraction failed"
            ) from e
    except Exception as e:
        logger.error(f"Unexpected error during XML download/extraction: {e}")
        if xml_path.exists():
            logger.warning("Falling back to existing (possibly stale) XML file")
        else:
            raise


def load_xml_data(file_path: Path) -> pd.DataFrame:
    """
    Parse XML financial disclosure metadata into structured DataFrame.

    Reads XML file using pandas XML parser with XPath to extract Member records.
    Each row represents a single financial disclosure filing with metadata including
    member name, filing date, document ID, and year.

    Args:
        file_path: Path object pointing to the XML metadata file

    Returns:
        DataFrame with columns from XML Member records (e.g., FilingDate, DocID, Year)

    Raises:
        FileNotFoundError: If the specified XML file doesn't exist
        ValueError: If XML is malformed, empty, or doesn't match expected schema
        xml.etree.ElementTree.ParseError: If XML syntax is invalid
    """
    try:
        df = pd.read_xml(file_path, xpath="//Member")
        if df.empty:
            raise ValueError("No data found in XML file")
        return df
    except Exception as e:
        raise ValueError(f"Failed to load XML file: {e}") from e


def process_filing_dates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Parse and validate filing date strings, removing invalid entries.

    Converts FilingDate string column to pandas datetime objects using the
    configured DATE_FORMAT. Invalid or unparseable dates are coerced to NaT
    (Not-a-Time) and then removed to ensure data quality.

    Args:
        df: Input DataFrame containing 'FilingDate' column with date strings

    Returns:
        DataFrame with FilingDate as datetime type, invalid dates removed

    Note:
        Modifies a copy of the input DataFrame, original is unchanged
    """
    df = df.copy()
    df["FilingDate"] = pd.to_datetime(
        df["FilingDate"], format=DATE_FORMAT, errors="coerce"
    )
    df = df.dropna(subset=["FilingDate"])
    return df


def filter_by_date_range(df: pd.DataFrame, days: int) -> pd.DataFrame:
    """
    Filter filings to include only those within specified time window.

    Creates a temporal filter based on the number of days to look back from
    the current date. Useful for focusing analysis on recent disclosure activity
    while ignoring older, potentially less relevant filings.

    Args:
        df: Input DataFrame with parsed 'FilingDate' datetime column
        days: Look-back period in days (0 or negative returns all records)

    Returns:
        DataFrame containing only filings within the specified date range

    Note:
        Uses pandas Timestamp for current date calculation
    """
    if days <= 0:
        return df
    # Calculate cutoff date based on current time
    current_date = pd.Timestamp.now()
    cutoff_date = current_date - pd.Timedelta(days=days)
    return df[df["FilingDate"] >= cutoff_date]


def sort_by_filing_date(df: pd.DataFrame, ascending: bool = False) -> pd.DataFrame:
    """
    Sort DataFrame by FilingDate.

    Args:
        df: Input DataFrame
        ascending: Sort order (False for latest first)

    Returns:
        Sorted DataFrame
    """
    return df.sort_values(by="FilingDate", ascending=ascending)


def build_pdf_urls(df: pd.DataFrame, base_url: str) -> pd.DataFrame:
    """
    Construct full PDF URLs from metadata components.

    Generates complete URLs for accessing financial disclosure PDF documents
    by combining the base URL with year and document ID. The resulting URLs
    point to publicly accessible PDF files on the House disclosure server.

    URL Pattern: {base_url}{Year}/{DocID}.pdf
    Example: https://disclosures-clerk.house.gov/.../2025/12345678.pdf

    Args:
        df: Input DataFrame containing 'Year' and 'DocID' columns
        base_url: Base URL prefix for the PDF document server

    Returns:
        DataFrame with new 'URL' column containing complete PDF URLs

    Note:
        Creates a copy of input DataFrame, original unchanged
    """
    df = df.copy()
    df["URL"] = (
        base_url + df["Year"].astype(str) + "/" + df["DocID"].astype(str) + ".pdf"
    )
    return df


def fetch_pdf_with_random_ua(url: str) -> Optional[bytes]:
    """
    Download PDF document with randomized user agent for reliability.

    Retrieves PDF content from the specified URL while rotating the User-Agent
    header to mimic various browsers. This helps avoid potential rate limiting
    or blocking by the server. Implements timeout protection and comprehensive
    error handling.

    Args:
        url: Complete URL to the PDF document

    Returns:
        PDF file content as bytes if successful, None if any error occurs

    Error Handling:
        Logs errors but returns None rather than raising exceptions, allowing
        batch processing to continue even if individual PDFs fail to download
    """
    try:
        ua = UserAgent()
        headers = {"User-Agent": ua.random}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    except Exception as e:
        logger.error(f"Failed to fetch {url}: {e}")
        return None


def pdf_to_base64_images(pdf_bytes: bytes) -> List[str]:
    """
    Convert PDF pages to base64-encoded PNG images for API transmission.

    Renders each PDF page as a PNG image, then encodes to base64 for efficient
    transmission to the OpenAI Vision API. Base64 encoding allows image data to
    be embedded directly in JSON payloads without separate file uploads.

    Process:
        1. Convert PDF bytes to image objects (one per page)
        2. Render each page as PNG format
        3. Encode PNG bytes to base64 strings

    Args:
        pdf_bytes: Raw PDF file content as bytes

    Returns:
        List of base64-encoded image strings (one per PDF page)
        Empty list if conversion fails

    Raises:
        ImportError: If pdf2image library is not installed

    Note:
        Requires pdf2image package and poppler-utils system dependency
    """
    if not PDF2IMAGE_AVAILABLE:
        raise ImportError(
            "pdf2image not available. Install with: pip install pdf2image"
        )

    try:
        images = convert_from_bytes(pdf_bytes)
        base64_images = []
        for img in images:
            buffer = io.BytesIO()
            img.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
            base64_images.append(img_base64)
        return base64_images
    except Exception as e:
        logger.error(f"Failed to convert PDF to images: {e}")
        return []


def analyze_pdf_with_openai(base64_images: List[str], url: str) -> str:
    """
    Extract financial information from PDF images using AI vision analysis.

    Sends PDF page images to OpenAI's vision-capable model to identify and
    extract stock transaction data, company names, ticker symbols, and other
    financial information through intelligent pattern recognition.

    This is a legacy single-document analysis function. For batch processing,
    use analyze_all_filings_at_once() instead.

    Args:
        base64_images: List of base64-encoded PNG images (PDF pages)
        url: Source PDF URL for logging/context (not sent to API)

    Returns:
        Text analysis summary from OpenAI, or error message if failed

    Note:
        - Processes first 5 pages only to manage token costs
        - Uses randomized user agent for API requests
        - Temperature set to 0 for deterministic/consistent output
    """
    # Retrieve API key from environment variable (required for OpenAI access)
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return "ERROR: OPENAI_API_KEY environment variable not set"

    ua = UserAgent()

    # Detailed analysis prompt for AI vision model
    # Instructs the model to extract specific financial data points
    prompt = """
    Analyze this financial disclosure document image(s) and extract information about:
    1. Any public companies mentioned
    2. Stock purchases, sales, or holdings
    3. Company names and ticker symbols if available
    4. Transaction details (buy/sell, amounts, dates if visible)

    Provide a concise summary. If no stock information is found, state "No stock transactions identified."
    """

    # Construct API message with text prompt and image data
    messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]

    # Append up to 5 PDF pages as images (cost management)
    for img_base64 in base64_images[:5]:  # Limit to first 5 pages to control API costs
        messages[0]["content"].append(
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{img_base64}"},
            }
        )

    # Configure OpenAI API request
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "User-Agent": ua.random,  # Randomize UA for reliability
    }
    data = {
        "model": OPENAI_MODEL,  # Vision-capable model
        "messages": messages,
        "max_tokens": 1000000,  # Allow comprehensive responses
        "temperature": 0,  # Deterministic output (no randomness)
        "top_p": 1,  # Use full probability distribution
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"OpenAI API error: {e}"


def analyze_filings(df: pd.DataFrame) -> List[Tuple[str, str]]:
    """
    Analyze PDF filings for stock information.

    Args:
        df: DataFrame with URL column

    Returns:
        List of (url, analysis) tuples
    """
    results = []
    for _, row in df.iterrows():
        url = row["URL"]
        logger.info(f"Analyzing {url}...")

        pdf_bytes = fetch_pdf_with_random_ua(url)
        if not pdf_bytes:
            results.append((url, "Failed to fetch PDF"))
            continue

        base64_images = pdf_to_base64_images(pdf_bytes)
        if not base64_images:
            results.append((url, "Failed to convert PDF to images"))
            continue

        analysis = analyze_pdf_with_openai(base64_images, url)
        results.append((url, analysis))

    return results


def fetch_and_convert_pdf(row):
    """
    Worker function for parallel PDF processing in thread pool.

    Downloads PDF from URL in DataFrame row and converts to base64 images.
    Designed to be executed concurrently by ThreadPoolExecutor for efficient
    batch processing of multiple PDFs.

    Args:
        row: DataFrame row containing 'URL' column with PDF link

    Returns:
        Tuple of (url, base64_images) where base64_images is a list of strings
        Returns (url, None) if download or conversion fails

    Note:
        Limits to first 3 pages per PDF to balance detail and performance
    """
    url = row["URL"]
    logger.info(f"Fetching: {url}")

    pdf_bytes = fetch_pdf_with_random_ua(url)
    if not pdf_bytes:
        logger.error(f"Failed to fetch {url}")
        return (url, None)

    base64_images = pdf_to_base64_images(pdf_bytes)
    if not base64_images:
        logger.error(f"Failed to convert {url} to images")
        return (url, None)

    logger.info(f"Added {len(base64_images[:3])} images from {url}")
    return (url, base64_images[:3])  # Limit to 3 pages per PDF


def analyze_all_filings_at_once(df: pd.DataFrame) -> str:
    """
    Batch-process multiple PDF filings with comprehensive AI analysis.

    Orchestrates parallel PDF fetching and conversion, then sends all images
    to OpenAI in a single API call for holistic cross-document analysis. This
    approach is more efficient than individual document analysis and enables
    the AI to identify patterns and connections across multiple filings.

    Workflow:
        1. Fetch and convert all PDFs in parallel (multi-threaded)
        2. Collect all base64 images into single payload
        3. Send consolidated batch to OpenAI Vision API
        4. Receive comprehensive cross-document analysis

    Args:
        df: DataFrame with 'URL' column containing PDF links

    Returns:
        Comprehensive analysis summary covering all documents
        Error message if API key missing or processing fails

    Performance:
        - Uses ThreadPoolExecutor with 20 workers for concurrent downloads
        - Limits to 3 pages per PDF and 20 total images for cost control
        - Extended timeout (120s) for large batch processing
    """
    # Retrieve API key from environment (security best practice)
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return "ERROR: OPENAI_API_KEY environment variable not set"

    ua = UserAgent()

    # Parallel PDF processing using thread pool for performance
    all_images = []
    successful_fetches = 0

    logger.info(f"Fetching and converting {len(df)} PDFs in parallel (20 threads)...")

    # Execute PDF fetching concurrently for significant performance improvement
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        # Submit all PDF fetch/convert tasks to thread pool
        future_to_row = {
            executor.submit(fetch_and_convert_pdf, row): row for _, row in df.iterrows()
        }

        # Collect results as tasks complete (non-blocking)
        for future in concurrent.futures.as_completed(future_to_row):
            url, images = future.result()
            if images:
                all_images.extend(images)
                successful_fetches += 1

    # Verify that at least some PDFs were successfully processed
    if not all_images:
        return "ERROR: No PDFs could be successfully fetched and converted to images."

    logger.info(
        f"Successfully processed {successful_fetches}/{len(df)} PDFs â†’ {len(all_images)} total images"
    )

    # Comprehensive analysis prompt for batch processing
    # Instructs AI to synthesize information across all documents
    prompt = f"""
    Analyze ALL these financial disclosure document images from {len(df)} different filings and provide a comprehensive summary:

    1. Identify all public companies mentioned across all documents
    2. List all stock purchases, sales, or holdings found
    3. Extract company names and ticker symbols
    4. Detail any transaction information (buy/sell, amounts, dates)
    5. Group holdings by company and calculate totals where possible
    6. Highlight any notable patterns, large transactions, or concentrations

    Provide a detailed, well-organized summary covering ALL the documents shown.
    If no stock information is found, clearly state this.
    """

    # Build API request with text instruction and all images
    messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]

    # Append images to message content (limit for cost/token management)
    for img_base64 in all_images[:20]:  # Cap at 20 images to stay within token limits
        messages[0]["content"].append(
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{img_base64}"},
            }
        )

    # Configure API request for batch analysis
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "User-Agent": ua.random,  # Rotate user agent
    }
    data = {
        "model": OPENAI_MODEL,  # Vision-capable model required
        "messages": messages,
        "max_tokens": 20000,  # Higher limit for comprehensive multi-document summary
        "temperature": 0,  # Deterministic output for consistency
        "top_p": 1,  # Full probability distribution
    }

    try:
        logger.info(
            f"Sending {len(all_images[:20])} images to OpenAI Vision API for analysis..."
        )
        response = requests.post(
            url, headers=headers, json=data, timeout=120
        )  # Extended timeout for large payloads
        response.raise_for_status()  # Raise HTTPError for bad responses
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except requests.exceptions.Timeout:
        return "ERROR: OpenAI API request timed out after 120 seconds"
    except requests.exceptions.HTTPError as e:
        return f"ERROR: OpenAI API HTTP error: {e}"
    except Exception as e:
        logger.error(f"Unexpected error during OpenAI API call: {e}")
        return f"ERROR: OpenAI API error during comprehensive analysis: {e}"


def summarize_all_analyses(analyses: List[Tuple[str, str]]) -> str:
    """
    Legacy function: Generate meta-summary from individual analyses.

    NOTE: This function is deprecated in favor of analyze_all_filings_at_once()
    which provides superior results by analyzing all images together rather than
    summarizing pre-analyzed text.

    Creates a higher-level summary by asking OpenAI to synthesize multiple
    individual document analyses into a cohesive overview.

    Args:
        analyses: List of (url, analysis_text) tuples from individual analyses

    Returns:
        Consolidated summary text, or error message if API call fails

    Deprecated:
        Use analyze_all_filings_at_once() for better results
    """
    # Get API key from environment
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return "ERROR: OPENAI_API_KEY environment variable not set"

    ua = UserAgent()

    prompt = """
    Summarize the stock information from these financial disclosure analyses.
    Provide a comprehensive overview of all stock transactions, holdings, and companies mentioned across all documents.
    Group by company if possible, and highlight any notable patterns or large transactions.
    If no stock information is found in any, state "No stock transactions identified in any filings."
    """

    content = "\n\n".join(
        [f"Document: {url}\nAnalysis: {analysis}" for url, analysis in analyses]
    )

    messages = [{"role": "user", "content": content}]

    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "User-Agent": ua.random,
    }
    data = {"model": OPENAI_MODEL, "messages": messages, "max_tokens": 1500}

    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"OpenAI API error during summarization: {e}"


def display_results(
    df: pd.DataFrame,
    top_n: Optional[int],
    days: int,
    overall_summary: Optional[str] = None,
) -> None:
    """
    Present analysis results in formatted console output.

    Displays a tabulated view of processed filings, lists all PDF URLs,
    and prints the comprehensive AI analysis summary. Provides clear,
    organized output for easy review of findings.

    Args:
        df: Processed DataFrame with filing metadata and URLs
        top_n: Maximum number of records to display (None shows all)
        days: Look-back period used for filtering (for informational message)
        overall_summary: AI-generated analysis text (if available)

    Output Format:
        1. Tabulated filing metadata
        2. List of PDF URLs
        3. Comprehensive stock analysis summary
    """
    # Determine how many records to display
    if top_n is not None:
        df_display = df.head(top_n)
        count = len(df_display)
        message = f"Showing top {count} of {len(df)} filings from last {days} days"
    else:
        df_display = df
        count = len(df_display)
        message = f"Showing all {count} filings from last {days} days"

    # Display formatted table of filing metadata
    logger.info("\n" + "=" * 80)
    logger.info("FILING METADATA")
    logger.info("=" * 80)
    logger.info(tabulate(df_display, headers="keys", tablefmt="plain"))

    # Display complete list of PDF URLs for reference
    logger.info("\n" + "=" * 80)
    logger.info(f"PDF URLS - {message} (sorted by latest filing date)")
    logger.info("=" * 80)
    for idx, url in enumerate(df_display["URL"], 1):
        logger.info(f"{idx}. {url}")

    # Display AI-generated analysis summary if available
    if overall_summary:
        logger.info("\n" + "=" * 80)
        logger.info("COMPREHENSIVE STOCK ANALYSIS SUMMARY")
        logger.info("=" * 80)
        logger.info(overall_summary)
        logger.info("\n" + "=" * 80)


def main() -> None:
    """
    Main application entry point and workflow orchestrator.

    Executes the complete financial disclosure analysis pipeline:
    1. Download/refresh XML metadata if needed
    2. Parse and filter disclosure records
    3. Fetch PDF documents in parallel
    4. Convert PDFs to images for AI analysis
    5. Analyze all documents with OpenAI Vision API
    6. Display comprehensive results

    Supports two execution modes:
    - Full mode: Complete analysis with OpenAI (requires API key)
    - Test mode: Data processing only, no API calls (use --test flag)

    Environment:
        OPENAI_API_KEY: Required for full analysis mode

    Command Line:
        python3 load_xml.py        # Full analysis
        python3 load_xml.py --test # Test mode (no API)
    """
    logger.info("=" * 80)
    logger.info("Congressional Financial Disclosure Analysis Tool v1.0.0")
    logger.info("=" * 80)
    logger.info("")

    # Download/update XML metadata if needed (checks freshness automatically)
    download_and_extract_xml()

    # Detect test mode from command line arguments
    test_mode = "--test" in sys.argv

    if test_mode:
        logger.info("TEST MODE ACTIVE")
        logger.info("Running data processing only, skipping OpenAI analysis")
        logger.info("")
        api_key = "test-key"  # Placeholder for test mode
    else:
        # Verify OpenAI API key is available (REQUIRED for full analysis)
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.error("OPENAI_API_KEY environment variable not set!")
            logger.error("")
            logger.error("To run full analysis, set your API key:")
            logger.error("  export OPENAI_API_KEY='your-api-key-here'")
            logger.error("")
            logger.error("Get your API key from: https://platform.openai.com/api-keys")
            logger.error("")
            logger.error("Alternatively, run in test mode (no API calls):")
            logger.error("  python3 load_xml.py --test")
            sys.exit(1)

        logger.info("OpenAI API key found")
        logger.info("")

    try:
        # STEP 1: Load and parse XML metadata
        logger.info("[1/7] Loading XML metadata...")
        download_and_extract_xml()  # Ensures XML is fresh
        df = load_xml_data(Path(DEFAULT_XML_FILE))
        logger.info(f"      Loaded {len(df)} total disclosure records")

        # STEP 2: Parse and validate filing dates
        logger.info("[2/7] Processing filing dates...")
        df = process_filing_dates(df)
        logger.info(f"      {len(df)} records with valid dates")

        # STEP 3: Apply temporal filter
        logger.info(f"[3/7] Filtering to last {DEFAULT_DAYS} days...")
        df = filter_by_date_range(df, DEFAULT_DAYS)
        logger.info(f"      {len(df)} recent filings match filter")

        if len(df) == 0:
            logger.error("")
            logger.error(f"No filings found in the last {DEFAULT_DAYS} days")
            logger.error("Try adjusting the DEFAULT_DAYS constant in the script")
            sys.exit(1)

        # STEP 4: Sort by recency (latest first)
        logger.info("[4/7] Sorting by filing date (latest first)...")
        df = sort_by_filing_date(df)

        # STEP 5: Generate PDF URLs from metadata
        logger.info("[5/7] Constructing PDF URLs...")
        df = build_pdf_urls(df, DEFAULT_BASE_URL)
        logger.info("      Sample URLs:")
        for i, url in enumerate(df["URL"].head(3)):
            logger.info(f"        {i + 1}. {url}")

        # TEST MODE: Skip API-dependent steps
        if test_mode:
            logger.info("")
            logger.info("[6/7] SKIPPED - PDF fetching (test mode)")
            logger.info("[7/7] SKIPPED - OpenAI analysis (test mode)")
            logger.info("")
            display_results(
                df,
                DEFAULT_TOP_N,
                DEFAULT_DAYS,
                "[TEST MODE] Analysis skipped - no API calls made",
            )
            logger.info("")
            logger.info("Test mode completed successfully")
            return

        # STEP 6: Verify required dependencies for full mode
        logger.info("[6/7] Verifying dependencies...")
        if not PDF2IMAGE_AVAILABLE:
            logger.error("")
            logger.error("Missing required package: pdf2image")
            logger.error("Install with: pip install pdf2image")
            logger.error("Also requires system package: poppler-utils")
            sys.exit(1)
        logger.info("      All dependencies available")

        # STEP 7: Execute comprehensive AI analysis
        logger.info("[7/7] Analyzing PDFs with OpenAI Vision API...")
        logger.info("      This may take several minutes for large batches...")
        overall_summary = analyze_all_filings_at_once(df)

        # Display final results
        logger.info("")
        logger.info("Analysis complete! Displaying results...")
        logger.info("")
        display_results(df, DEFAULT_TOP_N, DEFAULT_DAYS, overall_summary)

        logger.info("")
        logger.info("Analysis completed successfully")

    except KeyboardInterrupt:
        logger.error("")
        logger.error("Analysis interrupted by user (Ctrl+C)")
        sys.exit(130)  # Standard exit code for SIGINT
    except Exception as e:
        logger.error("")
        logger.error("=" * 80)
        logger.error("FATAL ERROR")
        logger.error("=" * 80)
        logger.error(f"Error: {e}")
        logger.error("")
        logger.error("Stack trace:")
        import traceback

        traceback.print_exc()
        logger.error("=" * 80)
        sys.exit(1)


if __name__ == "__main__":
    main()
